% Following magic comments allow for compilation of root file
% !TEX root = ../../../../temp_manuscript.tex

\chapter{Discussion}\label{chap:discussion}
\begin{ChapterAbstractNoTitle}
\end{ChapterAbstractNoTitle}
In this thesis, I showed the potential of automated methods for the image analysis of glioma in several different ways.
Most prominently, I developed machine learning methods that predict the genetic status of glioma from preoperative \gls{MRI} scans (\cref{chap:LGG1p19q,chap:prognosais}).
I also used computational methods to evaluate potential observer-independent imaging markers (\cref{chap:HGGLocation,chap:LGGLocation}), developed machine learning methods to help with the initial stages of research (\cref{chap:DDS}), and the segmentation of glioma (\cref{chap:prognosais}).
In this chapter, I discuss the main findings and contributions of this thesis (\cref{sec:discussion_main_findings}), discuss the the clinical and technical opportunities and challenges of radiomics (\cref{sec:discussion_clinical,sec:discussion_technical}), provide an outlook on the future clinical impact of these methods and potential future research directions (\cref{sec:discussion_future}), and end with a general conclusion (\cref{sec:discussion_conclusion}).

\section{Main findings \& contributions}\label{sec:discussion_main_findings}

\subsection{Summary of findings}

In \cref{chap:HGGLocation,chap:LGGLocation} I looked at the relation between the location of \gls{glioma} and their genetic status.
In \cref{chap:LGGLocation} I found that \gls{IDH}-mutated \gls{LGG} are more frequently located in the frontal lobes and \gls{IDH} wildtype \gls{LGG} are more frequently located in the basal ganglia of the right hemipshere.
In \cref{chap:HGGLocation} I found that there was no difference in localization between \gls{MGMT} methylated and \gls{MGMT} unmethylated \gls{IDH} wildtype glioablastoma.


In \cref{chap:LGG1p19q} I developed a radiomics method that predicts the 1p/19q status of presumed \gls{LGG} using pre-operative \gls{T1C} and \gls{T2} scans.
This method was able to non-invasively predict the 1p/19q status, outperforming the clinical experts that performed the same task in terms of accuracy.

In \cref{chap:DDS} I developed a method that can automatically recognize and sort brain \gls{MRI} scans.
This method could sort the scans with a near-perfect accuracy, and even generalized to brain scans of patients without a \gls{tumor}.

Finally, in \cref{chap:prognosais} I developed a method that was able to predict the \gls{IDH} mutation status, 1p/19q co-deletion status, and grade of glioma, while simultaneously providing an automatic segmentation of the \gls{tumor}.
I showed that this method was able to perform all of these tasks with good performance, worked for all subtypes of glioma, and generalized well to a different patient population.


\subsection{Clinical and research impact}

The results from \cref{chap:LGGLocation,chap:HGGLocation} are the most straightforward to directly implement in the clinic.
The localization features that were found in those chapters for the different genetic groups (or in the case of the \gls{MGMT} status, the lack of such features), are easily to evaluate and do not require additional methods or expertise.
This shows the importance of doing research on these relatively simple imaging markers, as they can (in the short term) be the most impactful.

The radiomics research in \cref{chap:LGG1p19q,chap:prognosais} shows promising results, and constitutes an important step for bringing radiomics methods closer to the clinic.
In both cases an independent test set was used to validate the results of the studies, something that, although often suggested, is still not commonplace in (glioma) radiomics research [REFS].
In this way, these studies give a realistic, albeit perhaps less exciting, view on the promise of radiomics in the clinic.

In \cref{chap:LGG1p19q} there was a focus on the explainability of the algorithm; by showing the relative feature importance and showing what the algorithm considered to be the best examples of the different categories, we were able to provide some insight into the method.
Furthermore, the feature importance and examples matched with imaging features that were already known in the literature, thus providing some trust in our method.

The scan sorting method presented in \cref{chap:DDS} was developed with a research application in mind.
At the moment, the method is already finding its way into research as it is being used to reduce the time needed to prepare new datasets for research on automated analysis methods.
Although the clinical impact may currently be limited, the introduction of automated methods in the clinic might change this.
These automated methods will make us of specific scan types, and our method can be used as a first step in this pipeline, to automatically identify the required scan types.

\cref{chap:prognosais} is an important milestone for radiomics research, as it present one of the first multi-task radiomics approaches, which is especially pioneering by combining the segmentation task and genetic feature prediction task, which are often seen as separate challenges.
Furthermore, the method was trained on the largest, most diverse glioma dataset to date, and I was able to develop the method such that it could use the full \gls{MRI} scans as an input.
Thus, although the clinical impact might be limited at this moment, it pushed some important research boundaries, making it more likely that these methods can be embedded in the clinical workflow in the future.


\subsection{Methodological contributions}
In \cref{chap:LGG1p19q} I developed PREDICT, a toolbox that can extract imaging features from biomedical scans and use a machine learning method to correlate these imaging features with the clinical characteristics of a patient.
This toolbox has been integrated into WORC, a generalized radiomics toolbox, which can extract a large number of imaging features and use multiple machine learning methods to find the best radiomics algorithm \autocite{mstarmans2020worc}.
In this way, the threshold for the development of these algorithms is lowered, and new research can be more easily set up.
These toolboxes are open source and available at \url{https://github.com/Svdvoort/PREDICT}, \url{https://github.com/Svdvoort/PREDICTFastr}, and \url{https://github.com/MStarmans91/WORC}.

In \cref{chap:DDS}, I developed \acrlong{DDS}, an algorithm that can be used to quickly sort an unstructured dataset, such that it can then be used by automatic methods.
The code and trained model are publicly available at \url{https://github.com/Svdvoort/DeepDicomSort}, allowing other research to easily use this method in their own research.


In \cref{chap:prognosais} I developed the PrognosAIs toolbox.
The goal of this toolbox was to allow researchers to focus on designing new deep learning architectures and quickly setting up experiments, while ensuring that the computational resources were used as efficiently as possible
For example, PrognosAIs can automatically train deep learning networks on parallel GPUs, and tunes the training of the algorithm to the specific capabilities of the GPU to reduce the memory footprint.
In this way, researchers do not have to focus on optimizing their code, instead focussing on the actual experiments.



\section{How will radiomics find its way into the clinic?}\label{sec:discussion_clinical}

Although radiomics methods, such as those presented in \cref{chap:LGG1p19q,chap:prognosais}, could (theoretically) be implemented in the clinic at this moment, radiomics methods are for the most part restricted to the research domain.
In this section I discuss some of the barriers that radiomics methods still have to overcome before they can find widespread clinical acceptance of radiomics methods.

\subsection{What is the added value of radiomics?}\label{subsec:discussion_added_value_radiomics}

Most radiomics methods have the same goal: the prediction of certain clinical characteristics based on biomedical images.
However, the implementation of these methods in the clinical workflow is not always straightforward.
This can be mainly attributed to the unclear potential of these methods to change the clinical workflow and improve the patient treatment.
For example, the methods developed in \cref{chap:LGG1p19q,chap:prognosais} could be used to pre-operatively predict the histological and genetic features of glioma.
However, when it is discovered that a patient has a brain \gls{tumor}, the tumor will be resected in a large majority of the cases as part of the standard clinical workflow [REFS].
Thus, the genetic and histological features can be determined from this tissue.
This analysis is more reliable than the radiomics methods, and since it is part of the standard treatment route, it is unlikely that radiomics methods will replace or change this workflow in most cases.
How then do we fit radiomics methods into the clinical workflow, and what is the added value of radiomics methods?

Although a majority of the patient will still undergo the traditional pathway of resection, there might be cases for which radiomics methods can be very useful.
For example, if a \gls{tumor} is located at a location which makes it hard to operate on, and the patient shows no other symptoms, the risk of a biopsy might be too large compared to the benefits.
Thus, radiomics methods may need to be optimized for these kinds of cases, to ensure that the performance is optimal.

Secondly, errors can be made in the biopsy of glioma, in which case a radiomics method can serve as a second opinion.
A biopsy sample, which only presents a small part of the \gls{tumor}, could be taken from a region of the tumor that is less aggressive, giving a wrong representation for the \gls{tumor} as a whole.
Radiomics methods can actually take into account the full \gls{tumor} from the \gls{MRI} scan, and might pick up on the more aggressive parts as well.
Thus, the radiomics methods could be used as a second opinion.
If there is an disagreement between the pathological analysis and the radiomics prediction, it might warrant a second look.

Thirdly, radiomics methods might be an excellent opportunity to include  the latest glioma insights into clinical workflows in which the current standard-of-care cannot be used.
For example, although genetic analysis is seen as the golden standard method which should be used in every clinical workflow of glioma patients, it is a luxury to have the required equipment and expertise.
Thus, not all institutes have the resources required to use the golden standard methods, especially in underprivileged countries \autocite{santosh2019india}.
With the addition of more genetic features in the glioma categorization, the price of the equipment needed to analyze these features  increases, and even for privileged institutes that have access to the required equipment it might not be cost-effective to analyze all \glspl{tumor} \autocite{malzkorn2016practical,dewitt2017costIDH}.
This makes these golden standard methods, although preferable for their accuracy, infeasible as the de facto standard everywhere.
In these cases, radiomics methods can be a cheap and easy-to-use solution, which, although not as accurate as the golden standard methods, can improve over the current situation in which there might be no or very limited information about the molecular and histological features.


\subsection{How do we create realistic expectations for radiomics methods?}

The rapid rise in popularity of radiomics methods can partially be attributed to the promise that these methods can achieve near-perfect performances.
However, as was quickly realized when more thorough studies of these methods were carried out, the performance that was promised was not reached in the clinical reality.
The high expectations of and consequent disappointment in  radiomics methods has led to skepticism.
How then do we create realistic expectations for radiomics methods, showing their true potential without overselling it?

One key factor is the proper validation of these methods.
Firstly, this means that studies should use independent datasets to validate the performance of these methods [REFS].
Currently, this is done only in a minority of the studies and as a result the reported performance is often too optimistic for the actual clinical performance.
Secondly, it is important to report the metrics in a proper way and for the relevant groups.
The performance of radiomics methods can be expressed in numerous metrics, where each might give a different view on the method's performance.
This can lead to unconsciously choosing the best performing metrics, which do not give a representation of the real performance.
Furthermore, based on the clinical characteristics of patients, the metrics might also be skewed and it is important to takes this into account.

It is also important to include radiomics methods in prospective studies.
At the moment, most studies use retrospective data, which presents two problems.
The retrospective data is often several years old and since imaging protocols or clinical pathways might have changed during this time, this influences the performance of the radiomics methods in the current clinical standard.
Prospective studies might also reveal new obstacles for the implementation of radiomics methods in the clinic, that were not considered in retrospective studies.
It is also a good way to evaluate in which cases radiomics methods could contribute something.

\subsection{When is radiomics performance good enough?}

Another important barrier for clinical acceptance of radiomics methods is their predictive performance.
Although the methods developed in \cref{chap:LGG1p19q,chap:prognosais} successfully predicted the genetic features, their performance was considered insufficient for clinical use.
However, there is no generally accepted threshold for sufficient clinical performance, which makes it hard to determine what the main focus of performance improvement should be.
For example, it is possible to try and improve the overall performance, but in some cases it might be more interesting to improve the performance in specific subgroups, as discussed in \cref{subsec:discussion_added_value_radiomics}.
Thus, before these methods will be clinically accepted we need to determine when is the performance of radiomics methods good enough?

A first factor that plays a role is the observer-independency and speed of machine learning methods, which make it easy to measure their performance on different datasets.
This ease of performance measurement and abundance of reported achieved performances has led very concrete overview of the performance, something that is not possible for the golden standard methods.
Thus, this leads to an unfair comparison, where the performance of radiomics methods is compared to the performance of a methods from which we might not actually not know the performance.

Secondly, the performance of radiomics methods is always compared to a golden standard that is derived through some other method (e.g. through the analysis of a biopsy sample in the case of genetic features).
The machine learning methods will never (directly) outperform these golden standard methods, at best they are able to match them perfectly; in that sense, the performance will never be acceptable, since the currently available method is already better.
Of course, the radiomics methods have a different starting point from which they try to derive the same information (e.g. a scan instead of a tumor sample to predict the genetic features).
Thus, the radiomics methods might need to be compared against a different benchmark than always being compared to the golden standard ground truth.
For example, in \cref{chap:LGG1p19q}, we did not reach perfect performance compared to the ground truth obtained from tumor tissue analysis.
However, when we compared our method with clinical experts, who used the same data as was available to the algorithm, we found that our algorithm was able to outperform some of them.
Thus, based on this benchmark, the performance might already be acceptable for clinical use, since it can improve over a current comparable situation.

Lastly, the performance that needs to be achieved also depends very much on the intended use of the radiomics method.
This also means that the benchmark with which the radiomics methods are compared could differ between institutes.
For example, the experts with which we compared our method in \cref{chap:LGG1p19q} were highly specialized in glioma and came from an institute specialized in glioma.
Thus, it is likely that not all institutes have access to the same lever of expertise, which lowers the bar for clinically acceptable performance of radiomics methods in some institutes, and the performance that some of the current methods achieve might already improve the patient treatment.

\subsection{How do we increase trust in radiomics methods?}

Another important barrier for the acceptance of machine learning methods is their lack of interpretability.
Deep learning methods especially are considered black boxes since there is no explicit knowledge on what imaging features these methods base their prediction on.
Thus, it is hard to trust the predictions of these methods, especially when considering that the clinical experts and the patient probably want an explanation as to why a certain treatment decision is made.
Even if a method shows perfect performance, it might not be used by clinical expert if they do no trust the method.
How then do we increase the trust in radiomics methods?

An important step is the increased explainability of machine learning methods.
In \cref{chap:LGG1p19q,chap:prognosais,chap:DDS} we  tried to provide some insight into the machine learning methods.
In \cref{chap:prognosais,chap:DDS} we did so by showing attention maps and convolution filter outputs, and in \cref{chap:LGG1p19q} we looked at the relative importance of different features.
Although in this way we can provide some insight into the machine learning methods, this is only very globally and lacks an explanation as to why certain imaging features are used in the prediction.
Although a lot of research is focusing on methods that can provide additional insight into these methods, the question remains whether human-interpretable explanations of machine learning methods will be available in the near future \autocite{zhang2018interpretable}.
This creates distrust for these algorithms, since even if it can be shown that an algorithm is looking at the correct part of the scan (i.e. in the tumor and not somewhere outside the brain), the algorithm might consider it for the wrong reasons.
It is easy to attach our own explanation to these methods if we see them focusing on particular parts, but this can easily lead to misinterpretations.
A method that can explain why a deep learning network looks at a specific part of a scan would not only create more trust in the algorithms, as the predictions can now be verified, it can also help to develop new and better ones.
If a sample is wrongly predicted, but the algorithm can explain why it looked at certain parts, it might be possible to redesign the method to make sure that the method looks at the correct parts of the scans.

Another way to create more trust is to have methods that can not only predict certain clinical characteristics, but that are also able to provide the level of certainty with which they make their predictions.
If it can be shown that these methods are good at estimating their own uncertainty, this allows clinical experts to only take the predictions into account in cases where the algorithm is certain.
Although, this of course requires trust in the uncertainty prediction of the method, it would be easier to convince users, and would create a sense of trust since the method does not seem certain, shows that the method is also fallible.

\subsection{What clinical assumptions are put on radiomics methods?}

Since radiomics methods are in the end statistical methods, these methods rely on certain (implicit or explicit) assumptions on the input data.
For example, in this thesis the assumption was always made that the scans that were used actually were scans of glioma patients.
However, this means that it should first be established whether a scan contains a glioma.
Thus, the question becomes: what assumptions are made and how can we deal with these assumptions?

Firstly, it is important to consider, and explicitly state, what assumptions about the input data are made.
For example, in \cref{chap:LGG1p19q} we assumed that the scans were of presumed \gls{LGG}, based on the visual appearance of these glioma in the scan.
Thus, this adds another upstream analysis step which can cause errors in the input data, and it restricts the applicability of these algorithms, since clinical expertise is needed for the interpretation of the scans.
Therefore, in \cref{chap:prognosais} we made no such assumptions, instead we included any type of glioma.
This generalization reduces the amount of upstream steps, reducing the potential for errors made in the evaluation of the data.
However, a further generalization to also include non-glioma scans would mean that these algorithms need to predict multiple, unrelated outputs.
For example, one output can still be the \gls{IDH} mutation status, while another output is whether a scan shows a brain infarct, in which case the \gls{IDH} mutation is nonsensical.
Thus, the machine learning methods should be as generalizable as possible to reduce the amount of upstream analysis that needs to be done, while making sure that the predictions of the model remain coherent.
If the outputs are not coherent, the tasks should be split up into several models.


Thus, the methods presented in \cref{chap:LGG1p19q,chap:prognosais} are not fully automated, since these kinds of interpretations still manually need to be made upstream.
To make a fully automatic pipeline, machine learning algorithms can be developed that distinguish between different types of brain conditions, for example between glioma and metastasis \autocite{chen2019metastatic}.
This interpretation upstream can also result in non-glioma scans being fed into the machine learning algorithms, in which case it will of course still make its predictions, as it cannot indicate that the scan did not contain a glioma.



\section{What technical challenges are radiomics methods faced with?}\label{sec:discussion_technical}
Apart from the questions related to the clinical implementation of radiomics methods, there are also still several technical challenges to overcome for radiomics and automated methods.

\subsection{How can we keep up with new research?}
Radiomics methods are, in the end, statistical methods that rely on historical data to predict future events.
In order to properly extract enough statistical information, a database of sufficient size is needed from which the method can extract relevant information.
Since the amount of glioma patients is (luckily) relatively low, it often takes several years to accumulate a enough data.
This means that these database often contain data over several years, during which time research might have provided new insights into relevant data for glioma patients.
However, this data is then often impossible to collect, especially when it comes to the development of new imaging sequences, since patients cannot be scanned pre-operatively anymore.
Thus, radiomics methods are by their definition almost always based on outdated data.
The question then is, how can we make radiomics methods that can keep up with the latest clinical research?

First it is important to distinguish between clinical research that influences the data that is used as an input to radiomics methods and those that influence the clinical features that the algorithms try to predict.
The input is often imaging, thus research on new imaging sequences (see also \cref{sec:dicussion_new_imaging}) will have a large influence here.
For the clinical features, new insights into the genetics that play a role in certain patient groups will influence the results, see \cref{sec:discussion_new_genetics}

In the case of new imaging sequences, transfer learning might be a solution.
Transfer learning takes an already trained algorithm, and tries to use the knowledge of that algorithm to make the training of a new algorithm on slightly different, but related, data easier \autocite{shin2016transfer}.
In this way, methods that have already been trained can be used to train new algorithms with less effort, leveraging information that was already learned.

For the case of new insights into clinical features to predict, the situation is both more difficult, but also easier.
In the case of new genetics being discovered which need to be predicted, it might be possible to obtain this data for older patients.
If tumor tissue has been saved, it could be issued to analyze it for these new genetic features, although this might not be realistic for all patients, and could be a costly procedure.
Multi-task networks, such as the one presented in \cref{chap:prognosais}, might also be part of the solution.
By predicting multiple genetic and histological features simultaneously, the method was able to learn relationships between the different features.
If a new genetic feature is discovered, which correlates with existing features (i.e. if it occurs mostly in \gls{LGG}), the correlations already learned by the algorithm might lead to a faster training convergence, thus requiring less data, than training a model from scratch.
Thus, the trained weights from the algorithm can be used, while adding a new output for the new genetic status.


\subsection{What technical/hardware challenges are we faced with?}

An important technical limitation that deep learning methods face is the limited amount of memory available on GPUs, which constrains the size of deep learning models.
In \cref{chap:prognosais}, I developed a method that can fit a large model, while also using the full 3D scan as an input.
However, this model pushed the limits of the GPU memory, and the model could not be larger on the GPU available for this research.
On the one hand this limitation leads to a critical evaluation of what is essential in the model, cutting out the superfluous parts.
On the other hand, larger models might improve the performance of deep learning methods.

Larger models do not always have to contain merely more filters than their smaller counterparts, since more filters might not always lead to better results.
However, models can be extended in other ways, for example some models use the scan at different resolutions, thereby trying to force the model to learn features that are relevant at different scales \autocite{akkus20171p19q}.

Another interesting approach would be to keep the different scan modalities separate during the convolutions.
Currently, the first convolutional layer combines the different modalities into feature maps.
However, keeping the modalities separate deeper into the model may lead to different features being learned.
Of course, one can then also make different paths in the models for different combinations of modalities, where each path learn specific features.
However, this will significantly increase the amount of memory required to train these models.
A schematic of such a model is shown in \cref{fig:discussion_architecture}.
This model contains XXX parameters, XXX as much as the model presented in \cref{chap:prognosais}.

\begin{figure}
\includegraphics[width=\textwidth]{example-image-a}
\caption{Example architecture with pathways for different imaging modalities}\label{fig:discussion_architecture}
\end{figure}

The amount of GPU memory has been increasing, and in the past four years the amount of memory at the same pricepoint has doubled.
Furthermore, new innovations such as half-precision training and parallel training allow for bigger models to be trained using the same amount of memory.
The optimization of current models and methods to make use of these new features can already lead to the use of new model types, although the impact of larger models remains to be seen.
This advance in hardware raises an important ethical question.
If the best performing models can only run on the newest (most expensive) GPUs, this would be a big obstacle in research and in the clinical applicability of these methods.


\subsection{How do we deal with ground truth uncertainties?}

An important aspect for all machine learning methods is the uncertainty in the ground truth.
I distinguish two categories of uncertainty: The first category is an imperfect, but observer-independent ground truth method, the second category is an observer-dependency in the ground truth.

The genetic analysis of tumor tissue is imperfect (and also contains some observer dependency, although this could be eliminated).
For example, when comparing multiple methods to determine the \gls{IDH} status of \gls{tumor} tissue, these methods are not always in agreement \autocite{pyo2016concordance}.
As a result of this uncertainty in the golden standard, the ground truth used to train and evaluate machine learning methods probably contains errors.
Of course, in these cases it is still unknown what the \say{true} ground truth is, since there is no way too \per{100} accurately determine it, making it impossible to gain a true feeling for the uncertainty in the ground truth.
The uncertainty can be larger for some ground truths than for others, for example the accuracy for the analysis of the \gls{MGMT} status is quite low \autocite{wang2017mgmt}.
The uncertainty for a specific output is important to take into account during the development of a machine learning model, since it might not actually possible to properly train a model if the uncertainty in the ground truth is too large.

Another aspect is the observer-dependency of some ground truths.
For example, in the case of tumor segmentation it is not possible to determine a \say{true} ground truth, it will always depend on the observer that made it.
In cases where the ground truth is observer-dependent, striving for the best performing algorithm might be unrealistic, since the ground truth could be less accurate than the predictions.
For example, when comparing segmentation from multiple observers, a whole \gls{tumor} mean DICE score of 0.85 was found, which is very close our DICE score of 0.84 achieved in \cref{chap:prognosais} \autocite{menze2015brats}.
Therefore, it might be hard to improve the performance further, especially if ground truth segmentation of multiple observers are used to train the algorithm.
In this way, automated methods can be very helpful for \gls{tumor} segmentation, as they homogenize the \gls{tumor} segmentations.
However, what a segmentation contains is based on personal and institutional preferences, thus multiple methods might need to be trained to achieve the same goal of \gls{tumor} segmentation for different institutes.


\section{What can we expect in the future?}\label{sec:discussion_future}

The sections above have raised some important questions concerning the use of radiomics.
In this section, I present a look to the future.


\subsection{What can improvements in imaging bring to radiomics?}\label{sec:dicussion_new_imaging}

One of the big challenges for radiomics that use \gls{MRI} scans at the moment is the qualitative nature of \gls{MRI} scans, which does not allow for objective biomarkers.
In recent years quantitative \gls{MRI} has gained in popularity, where using new \gls{MR} sequences allows for the measurement of the true tissue values.
This means that voxel values in these quantitative \gls{MRI} scans now have a meaning, and that the same patient scanned on different scanners should result in the same scan.
This makes the development of new imaging biomarkers and automated methods easier, as they can now use the absolute voxel intensity.
However, quantitative \gls{MRI} sequences are not yet clinically used.

Another important development is the development of new \gls{MRI} sequences.
For example, \gls{DWI} and \gls{PWI} sequences which provide physiological information in addition to the structural information provided by sequences such as \gls{T1} and \gls{T2} used in this thesis.
It has been shown that adding these sequences in radiomics methods provides additional information, and leads to a better performance \autocite{park2020radiomicsdwi,kim2020radiomicsdwi}.
We did not include these sequences in the research in this thesis, as they were not as present four years ago as they are now, thus not enough data was available.
New \gls{MRI} sequences are constantly being developed, for example \gls{CEST}, which provides additional physiological data not present in \gls{PWI} and \gls{DWI}.



\subsection{What can improvements in genetics bring to radiomics?}\label{sec:discussion_new_genetics}

In \cref{chap:HGGLocation} we saw that we could find no relationship between the \gls{MGMT} status of a tumor and the location of the tumor within the brain.
Then, in \cref{chap:prognosais}, we also attempted to predict the \gls{MGMT} status from \gls{MRI} scans, now using a deep learning network, where we did not find a relationship with the \gls{MGMT} status either.
A recent study by \citeauthorref{mikkelsen2020mgmt} also could not find a relationship between the \gls{MGMT} status and radiomics features.
In literature, the number of studies that successfully predicted the MGMT methylation status is limited, especially in comparison with the prediction of other genetic statuses.
Studies that did manage to find a correlation might be biased by predicting the IDH status instead.
Therefore, an important questions remains whether the imaging features can determine the MGMT status.

An important development in the field of genomics is the commodity of more advanced genetic analysis machines.
With the introduction of next generation sequencing the results from the genetic analysis become more reliable, which allows for a better training of radiomics models, and also allows for more genes to be analyzed.
In this way, new genes can be discovered that correlate with the aggressiveness of the tumor, and in the future the categorization will most likely be solely dependent on molecular features.

In this thesis we assumed that the molecular and histological features of glioma were homogenous throughout the whole \gls{tumor}, this means that if a \gls{tumor} was considered for example \gls{IDH} wildtype, that the \gls{tumor} was \gls{IDH} wildtype everywhere.
However, it is known that the genetic and histological features can actually show intra-tumor heterogeneity \autocite{eder2014heterogeneity}.
Thus, radiomics methods might need to be developed that take into account this intra-tumor heterogeneity, this is known as voxelwise radiomics, where instead of predicting single label for the whole \gls{tumor}, a label per voxel is predicted.
The problem in this case is the acquisition of ground truth data.
Tumor tissue obtained through biopsy is often obtained from a single or limited amount of locations, thus not containing the information for the whole \gls{tumor}.
Although it is possible, albeit very resource intensive, to analyze the whole \gls{tumor} after resection, the problem there is then to link the analysis of the \gls{tumor} tissue back to the exact location in the \gls{MRI} scan.
Radiomics methods that do voxelwise classification using \gls{tumor} segmentations with different labels for different classes exist, which allow for a voxelwise prediction by training on homogeneous ground truth \autocite{yogananda20201p19q}.
Although these types of methods might be correct in their predictions, and could solve the problems that face the collection of the ground truth, it is at the moment impossible to accurately verify them.

\subsection{Where will radiomics be applied?}

Therefore, there might also need to be a shift in the way radiomics is positioned.
Currently, it is often presented as a method that can replace a certain (currently used) golden standard method or even replace clinical experts.
Although this might become a reality in the long run, at the moment this seems rather unlikely.
Perhaps radiomics is better viewed as a way to democratize clinical expertise.
The methods can be developed using the latest insights and expertise available at privileged institutes, and this knowledge can then be distributed to institutes that normally would not have access to it.



\subsection{How do we deal with data?}

Machine learning methods can only be as good as the data that is used to develop and evaluate them.
Thus, databases have been ever increasing, containing more and more varied data.
In the end, all of this data comes from patients that were treated in a clinic.
This data can help develop new methods that can improve healthcare for future patients, but on the other hand there are several ethical and financial questions surrounding the collection of this data.
Thus, the question becomes how can we properly deal with patient data for inclusion in studies?

The first question is one of privacy.
Clinical data is by its very nature very privacy sensitive, and it requires special attention to ensure that none of this information can be traced back to the patient.
Furthermore, the patient might not want their data to be used apart from the clinical purpose.

The second question is one of finances.
Since both the Dutch healthcare system and the Dutch research infrastructure is largely financed by taxpayer money, it could be said that the since the data can improve the research and the research can cut down on healthcare costs, data needs to be collected and made available to all Dutch researchers.
However, what then if data is to be shared with institutes in other countries?
This data does contain some financial value, so given it away for free while it has been gathered using taxpayer money might be undesirable.
However, not sharing this data might lead to the stagnation of new developments, and might restrict people from having access to the best treatment.
Thus, this becomes an ethical dilemma.

One of the first iniatives that should be started is automated data collection within each institute.

\subsection{What else can automated methods contribute?}

For future research, the image analysis of glioma needs to be seen in a broader context.
At the moment, most research starts from the point where the glioma scans are available.
However, as mentioned above, there are actually a lot of steps before that stage is even reached, see \cref{fig:discussion_pipeline_automatic}.
In a lot of these steps automatic and machine learning methods can play a role.
For example, methods can be trained that automatically detect artifacts in scans \autocite{kustner2018artifacts}.
A new scan can then be made while the patient is still in the scanner, or algorithms can be trained that can actually remove these artifacts.
In a similar way, a lot of other steps can automatically be carried out before a neuro-radiologist takes a first look at the scans, which can reduce the workload for the radiologist and can provide them with more consistent information by remove the observer-dependency, for example in the volume measurement of a \gls{glioma}.


\begin{figure}[htbp]
\includegraphics[width=\textwidth]{Figures/Pipeline.png}
\caption{Overview of some steps in which automated or machine learning methods can play a role from the scanner till the scan reaches the clinician}\label{fig:discussion_pipeline_automatic}
\end{figure}


\subsection{Should radiomics methods be unconstrained?}

A third way to look at the clinical future of radiomics is to evaluate whether these models might actually better predict the aggressiveness of \gls{glioma} than the ground truth molecular and histological features.
If enough data has been used to train a model, the model could pick up on some image features that predict \gls{tumor} aggressiveness, which is not correlated with a molecular feature.
In retrospective data one could link the predictions of the model with the survival of the patients, which might show patients in different groups than expected from the ground truth.
Ultimately, this could only be properly evaluated using a prospective study with stratified patients groups: one where the treatment is based on the radiomics model and one where the treatment is based on the golden standard ground truth.
However, this is a risky study, with little evidence being currently available that shows the potential benefits.

\section{Conclusion}\label{sec:discussion_conclusion}


The above discussion raises a number of important questions that need to be addressed in future research.

Perhaps the most important question is what the future clinical application of radiomics should be.
Although radiomics methods show promising results, the resources used to develop them would be wasted if they do not eventually find a clinical application.
Thus, there are a few aveneus of approach that can be taken.

First of all, the view in which radiomics is normally presented: to obviate the need for an otherwise intrusive or difficult surgery.
In the case of glioma it seems unlikely that radiomics methods will replace biopsies or prevent surgeries.
The majority of the \glspl{tumor} are removed quickly after discovery, and the benefits of a watch-and-wait approach are often a topic of discussion.
Thus, the question becomes do the current radiomics methods provide enough additional benefit to warrant their existence, and if not, then what should they include to become relevant?
This question is difficult to answer, as a lot of different disciplines are involved in the treatment of glioma, each with their own view of what is needed to advance the patient treatment.
However, a consensus is required to steer the field of radiomics in the right direction.

As mentioned above, radiomics can also be viewed as a way of democratizing the latest  developments in glioma research.
This would require models to be made publicly available, and requires guidelines on when and how these radiomics models can be used if no golden standard method is available.
These guidelines should be made as approachable as possible, to make sure that the use of these models has a low threshold.


The radiomics predictions might also be reconsidered.
For example, instead of predicting genetics, it might be more interesting to predict which parts of the \gls{tumor} are the most important to remove during the resection.
This is information that is currently not available from a ground truth model, and thus provides truly new information.
In the same way, methods could be developed that can help to determine the best course of treatment for a certain patient.
Although these types of research will be faced with methodological challenges, right now the most important step it to reach a clinical consensus, to ensure that the field of radiomics is pushed in the right direction.

All in all, radiomics and automated image analysis are definitely here to stay.
With the increasing healthcare costs, the increasing amounts of data, and the decreasing amount of time clinical experts can spend on a single patient, automatic methods offer a cheap solution that can easily deal with large amounts of data.
Although several methodological and social barriers still need to be overcome, the ultimate goal of improved patient healthcare should be kept as the focus both from the technological side, and from the clinical side.
Ultimately, we do need to move away from the human eye to more objective analysis methods, while at the same time making sure that these automated methods provide the information clinicians want and that the automated methods and clinical experts can see eye to eye about each other's value.
