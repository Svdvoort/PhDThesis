% Following magic comments allow for compilation of root file
% !TEX root = ../../../../temp_manuscript.tex

\chapter{Discussion}\label{chap:discussion}
\begin{ChapterAbstractNoTitle}
\end{ChapterAbstractNoTitle}
In this thesis, I showed the potential of automated methods for the image analysis of glioma in several different ways.
Most prominently, I developed machine learning methods that predict the genetic status of glioma from preoperative \gls{MRI} scans (\cref{chap:LGG1p19q,chap:prognosais}).
I also used computational methods to evaluate potential observer-independent imaging markers (\cref{chap:HGGLocation,chap:LGGLocation}), developed machine learning methods to help with the initial stages of research (\cref{chap:DDS}), and the segmentation of glioma (\cref{chap:prognosais}).
In this chapter, I discuss the main findings and contributions of this thesis (\cref{sec:discussion_main_findings}), discuss the the clinical and technical opportunities and challenges of radiomics (\cref{sec:discussion_clinical,sec:discussion_technical}), provide an outlook on the future clinical impact of these methods and potential future research directions (\cref{sec:discussion_future}), and end with a general conclusion (\cref{sec:discussion_conclusion}).

\section{Main findings \& contributions}\label{sec:discussion_main_findings}

\subsection{Summary of findings}

In \cref{chap:HGGLocation,chap:LGGLocation} I looked at the relation between the location of \gls{glioma} and their genetic status.
In \cref{chap:LGGLocation} I found that \gls{IDH}-mutated \gls{LGG} are more frequently located in the frontal lobes and \gls{IDH} wildtype \gls{LGG} are more frequently located in the basal ganglia of the right hemipshere.
In \cref{chap:HGGLocation} I found that there was no difference in localization between \gls{MGMT} methylated and \gls{MGMT} unmethylated \gls{IDH} wildtype glioablastoma.


In \cref{chap:LGG1p19q} I developed a radiomics method that predicts the 1p/19q status of presumed \gls{LGG} using pre-operative \gls{T1C} and \gls{T2} scans.
This method was able to non-invasively predict the 1p/19q status, outperforming the clinical experts that performed the same task in terms of accuracy.

In \cref{chap:DDS} I developed a method that can automatically recognize and sort brain \gls{MRI} scans.
This method could sort the scans with a near-perfect accuracy, and even generalized to brain scans of patients without a \gls{tumor}.

Finally, in \cref{chap:prognosais} I developed a method that was able to predict the \gls{IDH} mutation status, 1p/19q co-deletion status, and grade of glioma, while simultaneously providing an automatic segmentation of the \gls{tumor}.
I showed that this method was able to perform all of these tasks with good performance, worked for all subtypes of glioma, and generalized well to a different patient population.


\subsection{Clinical and research impact}

The results from \cref{chap:LGGLocation,chap:HGGLocation} are the most straightforward to directly implement in the clinic.
The localization features that were found in those chapters for the different genetic groups (or in the case of the \gls{MGMT} status, the lack of such features), are easily to evaluate and do not require additional methods or expertise.
This shows the importance of doing research on these relatively simple imaging markers, as they can (in the short term) be the most impactful.

The radiomics research in \cref{chap:LGG1p19q,chap:prognosais} shows promising results, and constitutes an important step for bringing radiomics methods closer to the clinic.
In both cases an independent test set was used to validate the results of the studies, something that, although often suggested, is still not commonplace in (glioma) radiomics research [REFS].
In this way, these studies give a realistic, albeit perhaps less exciting, view on the promise of radiomics in the clinic.

In \cref{chap:LGG1p19q} there was a focus on the explainability of the algorithm; by showing the relative feature importance and showing what the algorithm considered to be the best examples of the different categories, we were able to provide some insight into the method.
Furthermore, the feature importance and examples matched with imaging features that were already known in the literature, thus providing some trust in our method.

The scan sorting method presented in \cref{chap:DDS} was developed with a research application in mind.
At the moment, the method is already finding its way into research as it is being used to reduce the time needed to prepare new datasets for research on automated analysis methods.
Although the clinical impact may currently be limited, the introduction of automated methods in the clinic might change this.
These automated methods will make us of specific scan types, and our method can be used as a first step in this pipeline, to automatically identify the required scan types.

\cref{chap:prognosais} is an important milestone for radiomics research, as it present one of the first multi-task radiomics approaches, which is especially pioneering by combining the segmentation task and genetic feature prediction task, which are often seen as separate challenges.
Furthermore, the method was trained on the largest, most diverse glioma dataset to date, and I was able to develop the method such that it could use the full \gls{MRI} scans as an input.
Thus, although the clinical impact might be limited at this moment, it pushed some important research boundaries, making it more likely that these methods can be embedded in the clinical workflow in the future.


\subsection{Methodological contributions}
In \cref{chap:LGG1p19q} I developed PREDICT, a toolbox that can extract imaging features from biomedical scans and use a machine learning method to correlate these imaging features with the clinical characteristics of a patient.
This toolbox has been integrated into WORC, a generalized radiomics toolbox, which can extract a large number of imaging features and use multiple machine learning methods to find the best radiomics algorithm \autocite{mstarmans2020worc}.
In this way, the threshold for the development of these algorithms is lowered, and new research can be more easily set up.
These toolboxes are open source and available at \url{https://github.com/Svdvoort/PREDICT}, \url{https://github.com/Svdvoort/PREDICTFastr}, and \url{https://github.com/MStarmans91/WORC}.

In \cref{chap:DDS}, I developed \acrlong{DDS}, an algorithm that can be used to quickly sort an unstructured dataset, such that it can then be used by automatic methods.
The code and trained model are publicly available at \url{https://github.com/Svdvoort/DeepDicomSort}, allowing other research to easily use this method in their own research.


In \cref{chap:prognosais} I developed the PrognosAIs toolbox.
The goal of this toolbox was to allow researchers to focus on designing new deep learning architectures and quickly setting up experiments, while ensuring that the computational resources were used as efficiently as possible
For example, PrognosAIs can automatically train deep learning networks on parallel GPUs, and tunes the training of the algorithm to the specific capabilities of the GPU to reduce the memory footprint.
In this way, researchers do not have to focus on optimizing their code, instead focussing on the actual experiments.



\section{How will radiomics find its way into the clinic?}\label{sec:discussion_clinical}

Although radiomics methods, such as those presented in \cref{chap:LGG1p19q,chap:prognosais}, could (theoretically) be implemented in the clinic at this moment, radiomics methods are for the most part restricted to the research domain.
In this section I discuss some of the barriers that radiomics methods still have to overcome before they can find widespread clinical acceptance of radiomics methods.

\subsection{What is the added value of radiomics?}\label{subsec:discussion_added_value_radiomics}

Most radiomics methods have the same goal: the prediction of certain clinical characteristics based on biomedical images.
However, the implementation of these methods in the clinical workflow is not always straightforward.
This can be mainly attributed to the unclear potential of these methods to change the clinical workflow and improve the patient treatment.
For example, the methods developed in \cref{chap:LGG1p19q,chap:prognosais} could be used to pre-operatively predict the histological and genetic features of glioma.
However, when it is discovered that a patient has a brain \gls{tumor}, the tumor will be resected in a large majority of the cases as part of the standard clinical workflow [REFS].
Thus, the genetic and histological features can be determined from this tissue.
This analysis is more reliable than the radiomics methods, and since it is part of the standard treatment route, it is unlikely that radiomics methods will replace or change this workflow in most cases.
How then do we fit radiomics methods into the clinical workflow, and what is the added value of radiomics methods?

Although a majority of the patient will still undergo the traditional pathway of resection, there might be cases for which radiomics methods can be very useful.
For example, if a \gls{tumor} is located at a location which makes it hard to operate on, and the patient shows no other symptoms, the risk of a biopsy might be too large compared to the benefits.
Thus, radiomics methods may need to be optimized for these kinds of cases, to ensure that the performance is optimal.

Secondly, errors can be made in the biopsy of glioma, in which case a radiomics method can serve as a second opinion.
A biopsy sample, which only presents a small part of the \gls{tumor}, could be taken from a region of the tumor that is less aggressive, giving a wrong representation for the \gls{tumor} as a whole.
Radiomics methods can actually take into account the full \gls{tumor} from the \gls{MRI} scan, and might pick up on the more aggressive parts as well.
Thus, the radiomics methods could be used as a second opinion.
If there is an disagreement between the pathological analysis and the radiomics prediction, it might warrant a second look.

Thirdly, radiomics methods might be an excellent opportunity to include  the latest glioma insights into clinical workflows in which the current standard-of-care cannot be used.
For example, although genetic analysis is seen as the golden standard method which should be used in every clinical workflow of glioma patients, it is a luxury to have the required equipment and expertise.
Thus, not all institutes have the resources required to use the golden standard methods, especially in underprivileged countries \autocite{santosh2019india}.
With the addition of more genetic features in the glioma categorization, the price of the equipment needed to analyze these features  increases, and even for privileged institutes that have access to the required equipment it might not be cost-effective to analyze all \glspl{tumor} \autocite{malzkorn2016practical,dewitt2017costIDH}.
This makes these golden standard methods, although preferable for their accuracy, infeasible as the de facto standard everywhere.
In these cases, radiomics methods can be a cheap and easy-to-use solution, which, although not as accurate as the golden standard methods, can improve over the current situation in which there might be no or very limited information about the molecular and histological features.

The radiomics predictions might also be reconsidered.
For example, instead of predicting genetics, it might be more interesting to predict which parts of the \gls{tumor} are the most important to remove during the resection.
This is information that is currently not available from a ground truth model, and thus provides truly new information.
In the same way, methods could be developed that can help to determine the best course of treatment for a certain patient.
Although these types of research will be faced with methodological challenges, right now the most important step it to reach a clinical consensus, to ensure that the field of radiomics is pushed in the right direction.

\subsection{How do we create realistic expectations for radiomics methods?}

The rapid rise in popularity of radiomics methods can partially be attributed to the promise that these methods can achieve near-perfect performances.
However, as was quickly realized when more thorough studies of these methods were carried out, the performance that was promised was not reached in the clinical reality.
The high expectations of and consequent disappointment in  radiomics methods has led to skepticism.
How then do we create realistic expectations for radiomics methods, showing their true potential without overselling it?

One key factor is the proper validation of these methods.
Firstly, this means that studies should use independent datasets to validate the performance of these methods [REFS].
Currently, this is done only in a minority of the studies and as a result the reported performance is often too optimistic for the actual clinical performance.
Secondly, it is important to report the metrics in a proper way and for the relevant groups.
The performance of radiomics methods can be expressed in numerous metrics, where each might give a different view on the method's performance.
This can lead to unconsciously choosing the best performing metrics, which do not give a representation of the real performance.
Furthermore, based on the clinical characteristics of patients, the metrics might also be skewed and it is important to takes this into account.

It is also important to include radiomics methods in prospective studies.
At the moment, most studies use retrospective data, which presents two problems.
The retrospective data is often several years old and since imaging protocols or clinical pathways might have changed during this time, this influences the performance of the radiomics methods in the current clinical standard.
Prospective studies might also reveal new obstacles for the implementation of radiomics methods in the clinic, that were not considered in retrospective studies.
It is also a good way to evaluate in which cases radiomics methods could contribute something.

\subsection{When is radiomics performance good enough?}\label{subsec:discussion_radiomics_performance}

Another important barrier for clinical acceptance of radiomics methods is their predictive performance.
Although the methods developed in \cref{chap:LGG1p19q,chap:prognosais} successfully predicted the genetic features, their performance was considered insufficient for clinical use.
However, there is no generally accepted threshold for sufficient clinical performance, which makes it hard to determine what the main focus of performance improvement should be.
For example, it is possible to try and improve the overall performance, but in some cases it might be more interesting to improve the performance in specific subgroups, as discussed in \cref{subsec:discussion_added_value_radiomics}.
Thus, before these methods will be clinically accepted we need to determine when is the performance of radiomics methods good enough?

A first factor that plays a role is the observer-independency and speed of machine learning methods, which make it easy to measure their performance on different datasets.
This ease of performance measurement and abundance of reported achieved performances has led very concrete overview of the performance, something that is not possible for the golden standard methods.
Thus, this leads to an unfair comparison, where the performance of radiomics methods is compared to the performance of a methods from which we might not actually not know the performance.

Secondly, the performance of radiomics methods is always compared to a golden standard that is derived through some other method (e.g. through the analysis of a biopsy sample in the case of genetic features).
The machine learning methods will never (directly) outperform these golden standard methods, at best they are able to match them perfectly; in that sense, the performance will never be acceptable, since the currently available method is already better.
Of course, the radiomics methods have a different starting point from which they try to derive the same information (e.g. a scan instead of a tumor sample to predict the genetic features).
Thus, the radiomics methods might need to be compared against a different benchmark than always being compared to the golden standard ground truth.
For example, in \cref{chap:LGG1p19q}, we did not reach perfect performance compared to the ground truth obtained from tumor tissue analysis.
However, when we compared our method with clinical experts, who used the same data as was available to the algorithm, we found that our algorithm was able to outperform some of them.
Thus, based on this benchmark, the performance might already be acceptable for clinical use, since it can improve over a current comparable situation.

Lastly, the performance that needs to be achieved also depends very much on the intended use of the radiomics method.
This also means that the benchmark with which the radiomics methods are compared could differ between institutes.
For example, the experts with which we compared our method in \cref{chap:LGG1p19q} were highly specialized in glioma and came from an institute specialized in glioma.
Thus, it is likely that not all institutes have access to the same lever of expertise, which lowers the bar for clinically acceptable performance of radiomics methods in some institutes, and the performance that some of the current methods achieve might already improve the patient treatment.

\subsection{How do we increase trust in radiomics methods?}

Another important barrier for the acceptance of machine learning methods is their lack of interpretability.
Deep learning methods especially are considered black boxes since there is no explicit knowledge on what imaging features these methods base their prediction on.
Thus, it is hard to trust the predictions of these methods, especially when considering that the clinical experts and the patient probably want an explanation as to why a certain treatment decision is made.
Even if a method shows perfect performance, it might not be used by clinical expert if they do no trust the method.
How then do we increase the trust in radiomics methods?

An important step is the increased explainability of machine learning methods.
In \cref{chap:LGG1p19q,chap:prognosais,chap:DDS} we  tried to provide some insight into the machine learning methods.
In \cref{chap:prognosais,chap:DDS} we did so by showing attention maps and convolution filter outputs, and in \cref{chap:LGG1p19q} we looked at the relative importance of different features.
Although in this way we can provide some insight into the machine learning methods, this is only very globally and lacks an explanation as to why certain imaging features are used in the prediction.
Although a lot of research is focusing on methods that can provide additional insight into these methods, the question remains whether human-interpretable explanations of machine learning methods will be available in the near future \autocite{zhang2018interpretable}.
This creates distrust for these algorithms, since even if it can be shown that an algorithm is looking at the correct part of the scan (i.e. in the tumor and not somewhere outside the brain), the algorithm might consider it for the wrong reasons.
It is easy to attach our own explanation to these methods if we see them focusing on particular parts, but this can easily lead to misinterpretations.
A method that can explain why a deep learning network looks at a specific part of a scan would not only create more trust in the algorithms, as the predictions can now be verified, it can also help to develop new and better ones.
If a sample is wrongly predicted, but the algorithm can explain why it looked at certain parts, it might be possible to redesign the method to make sure that the method looks at the correct parts of the scans.

Another way to create more trust is to have methods that can not only predict certain clinical characteristics, but that are also able to provide the level of certainty with which they make their predictions.
If it can be shown that these methods are good at estimating their own uncertainty, this allows clinical experts to only take the predictions into account in cases where the algorithm is certain.
Although, this of course requires trust in the uncertainty prediction of the method, it would be easier to convince users, and would create a sense of trust since the method does not seem certain, shows that the method is also fallible.

\subsection{What clinical assumptions are put on radiomics methods?}

Since radiomics methods are in the end statistical methods, these methods rely on certain (implicit or explicit) assumptions on the input data.
For example, in this thesis the assumption was always made that the scans that were used actually were scans of glioma patients.
However, this means that it should first be established whether a scan contains a glioma.
Thus, the question becomes: what assumptions are made and how can we deal with these assumptions?

Firstly, it is important to consider, and explicitly state, what assumptions about the input data are made.
For example, in \cref{chap:LGG1p19q} we assumed that the scans were of presumed \gls{LGG}, based on the visual appearance of these glioma in the scan.
Thus, this adds another upstream analysis step which can cause errors in the input data, and it restricts the applicability of these algorithms, since clinical expertise is needed for the interpretation of the scans.
Therefore, in \cref{chap:prognosais} we made no such assumptions, instead we included any type of glioma.
This generalization reduces the amount of upstream steps, reducing the potential for errors made in the evaluation of the data.
However, a further generalization to also include non-glioma scans would mean that these algorithms need to predict multiple, unrelated outputs.
For example, one output can still be the \gls{IDH} mutation status, while another output is whether a scan shows a brain infarct, in which case the \gls{IDH} mutation is nonsensical.
Thus, the machine learning methods should be as generalizable as possible to reduce the amount of upstream analysis that needs to be done, while making sure that the predictions of the model remain coherent.
If the outputs are not coherent, the tasks should be split up into several models.


Thus, the methods presented in \cref{chap:LGG1p19q,chap:prognosais} are not fully automated, since these kinds of interpretations still manually need to be made upstream.
To make a fully automatic pipeline, machine learning algorithms can be developed that distinguish between different types of brain conditions, for example between glioma and metastasis \autocite{chen2019metastatic}.
This interpretation upstream can also result in non-glioma scans being fed into the machine learning algorithms, in which case it will of course still make its predictions, as it cannot indicate that the scan did not contain a glioma.

The problem in this case is the acquisition of ground truth data.
Tumor tissue obtained through biopsy is often obtained from a single or limited amount of locations, thus not containing the information for the whole \gls{tumor}.
Although it is possible, albeit very resource intensive, to analyze the whole \gls{tumor} after resection, the problem there is then to link the analysis of the \gls{tumor} tissue back to the exact location in the \gls{MRI} scan.


\section{What technical challenges do radiomics need to overcome?}\label{sec:discussion_technical}
Apart from the questions related to the clinical implementation of radiomics methods, there are also several technical challenges to overcome for radiomics and automated methods.

\subsection{How can radiomics keep up with new clinical research?}
Radiomics methods are, at their core, statistical methods that rely on historical data to predict future events.
In order to extract enough statistical information, a database of sufficient size is needed.
Since the amount of glioma patients is (luckily) relatively low, it often takes several years to accumulate a large enough database.
For example, the data used in \cref{chap:prognosais} was collected over a period of ten years.
This means that these databases often contain data that is several years old.
During this time, research might have revealed new insights about the relevant clinical characteristics of patients, or new imaging sequences may have been developed.
These new insights cannot directly be incorporated, since the old data does not contain this new information.
Thus, radiomics methods are by their definition always based on outdated data.
The question then is: how can radiomics methods keep up with the latest clinical research?

It is important to distinguish between developments regarding the data that is used as an input to radiomics methods and developments that influence the clinical features that the algorithms try to predict.
Biomedical images are often used as an input for radiomics method, thus research on new imaging sequences (see also \cref{sec:dicussion_new_imaging}) will have a large influence here.
For the clinical features, new insights into the genetics that play a role in certain patient groups will influence the results, see \cref{sec:discussion_new_genetics}

When new imaging sequences are developed, enough data first needs to be collected such that methods can be trained using this new data.
However, patients that were previously treated cannot be scanned again, as the \gls{tumor} has often already been removed.
To minimize the amount of data that is need to training methods using these new imaging sequences transfer learning might be a solution.
Transfer learning takes an already trained algorithm, and uses the knowledge contained in that algorithm to make the training of a new algorithm on slightly different, but related, data easier \autocite{shin2016transfer}.

For the case of new insights into clinical features to predict, the situation is both more difficult, but easier at the same time.
If new genetic mutations are discovered which need to be included in the radiomics prediction, it might be possible to obtain this data for patients that were already included in the dataset.
If tumor tissue has been preserved, it can be analyzed for these new genetic features.
Although this might not be realistic for all patients, and could be a costly procedure, it might be worth the effort if the data can be used for multiple studies.
Multi-task networks, such as the one presented in \cref{chap:prognosais}, might also be part of the solution.
By predicting multiple genetic and histological features simultaneously, the method was able to learn relationships between the different features.
If a new genetic feature is discovered, which correlates with existing features (i.e. if it occurs mostly in \gls{LGG}), the correlations already learned by the algorithm might lead to a faster training convergence, thus requiring less data than training a model from scratch.
Thus, the trained weights from the algorithm can be used, while adding a new output for the new genetic status.


\subsection{What constrains model?}

An important technical limitation that deep learning methods face is the limited amount of memory available on GPUs, which constrains the size of deep learning models.
The method from \cref{chap:prognosais}, used a large neural network architecture while also using the full 3D scan as an input.
However, this model pushed the limits of the GPU memory, and the model could not be larger on the GPU available for this research.
On the one hand this limitation leads to a critical evaluation of what is essential in the model, cutting out the superfluous parts.
On the other hand, larger models might improve the performance of deep learning methods.

Larger models do not always have to contain merely more filters than their smaller counterparts, since more filters might not always lead to better results.
However, models can be extended in other ways, for example some models use the scan at different resolutions, thereby trying to force the model to learn features that are relevant at different scales \autocite{akkus20171p19q}.

Another interesting approach would be to keep the different scan modalities separate during the convolutions.
Currently, the first convolutional layer combines the different modalities into feature maps.
However, keeping the modalities separate deeper into the model may lead to different features being learned.
Of course, one can then also make different paths in the models for different combinations of modalities, where each path learn specific features.
However, this will significantly increase the amount of memory required to train these models.
A schematic of such a model is shown in \cref{fig:discussion_architecture}.
This model contains XXX parameters, XXX as much as the model presented in \cref{chap:prognosais}.

\begin{figure}
\includegraphics[width=\textwidth]{example-image-a}
\caption{Example architecture with pathways for different imaging modalities}\label{fig:discussion_architecture}
\end{figure}

The amount of GPU memory has been increasing, and in the past four years the amount of memory at the same pricepoint has doubled.
Furthermore, new innovations such as half-precision training and parallel training allow for bigger models to be trained using the same amount of memory.
The optimization of current models and methods to make use of these new features can already lead to the use of new model types, although the impact of larger models remains to be seen.
This advance in hardware raises an important ethical question.
If the best performing models can only run on the newest (most expensive) GPUs, this would be a big obstacle in research and in the clinical applicability of these methods.


\subsection{How will radiomics methods deal with ground truth uncertainties?}

As mentioned before, machine learning methods are statistical methods that learn from historical data.
However, this historical data can contain errors, causing the method to learn incorrect correlations.
Since this uncertainty in the ground truth is unavoidable, how can we make sure radiomics methods are able to handle these uncertainties?

For certain output categories, there might be a true ground truth, but there might not be method that can determine this ground truth with perfect accuracy.
For example, although a \gls{tumor} tissue sample expresses certain genetic mutations, the genetic analysis of tumor tissue is imperfect, thus the results may misrepresent the actual ground truth.
For example, when comparing multiple methods to determine the \gls{IDH} status of \gls{tumor} tissue, these methods are not always in agreement \autocite{pyo2016concordance}.
As a result of this uncertainty in the golden standard, the ground truth used to train and evaluate machine learning methods probably contains errors, but it is impossible to know which labels are wrong and which labels are correct.


The uncertainty can be larger for some ground truths than for others, for example the accuracy for the determination of the \gls{MGMT} status is quite low \autocite{wang2017mgmt}.
If there is a measure of uncertainty for a certain ground truth label, it can be used to use soft labels instead of hard labels.
In this way, the uncertainty is taken into account during the training.

In other cases, there might not be a true ground truth, the label may be inherently observer-dependent.
This is the case for \gls{tumor} segmentation, which will always depend on the observer that made it.
This makes it hard to define the best performing algorithm might be unrealistic, since the ground truth could be less accurate than the predictions.
For example, when comparing segmentation from multiple observers, a whole \gls{tumor} mean DICE score of 0.85 was found, which is very close our DICE score of 0.84 achieved in \cref{chap:prognosais} \autocite{menze2015brats}.
Therefore, it might be hard to improve the performance further, especially if ground truth segmentation of multiple observers are used to train the algorithm.
In this way, automated methods can be very helpful for \gls{tumor} segmentation, as they homogenize the \gls{tumor} segmentations.
However, what a segmentation contains is based on personal and institutional preferences, thus multiple methods might need to be trained to achieve the same goal of \gls{tumor} segmentation for different institutes.


\section{What does the future hold?}\label{sec:discussion_future}

The sections above have raised some important questions regarding the use of radiomics.
In this section, I present a look to the future.


\subsection{What can improvements in imaging bring to radiomics?}\label{sec:dicussion_new_imaging}

One of the big challenges for radiomics methods that use \gls{MRI} scans at the moment is the qualitative nature of \gls{MRI} scans, which does not allow for objective biomarkers.
In recent years quantitative \gls{MRI} has gained in popularity, where using new imaging sequences allows for the measurement of the true tissue values.
This means that voxel values in these quantitative \gls{MRI} scans now have a meaning, and that the same patient scanned on different scanners should result in the same scan.
This makes the development of new imaging biomarkers and automated methods easier, as the absolute voxel intensity can now be used.
However, quantitative \gls{MRI} sequences are still mainly a research application, not yet ingrained in the clinic.

New imaging sequences that measure physiological properties are also being developed, such as \gls{DWI} and \gls{PWI} sequences.
It has been shown that adding these sequences in radiomics methods provides additional information, and leads to a better performance \autocite{park2020radiomicsdwi,kim2020radiomicsdwi}.
In recent years these sequences have become more standard in clinical practice and sufficiently large databases are now available to use for the training of new methods.
More recent imaging sequences such as \gls{CEST}, provide additional physiological data not present in \gls{PWI} and \gls{DWI}, however they are far from common in clinical practice.



\subsection{What can improvements in genetics bring to radiomics?}\label{sec:discussion_new_genetics}
Although the \gls{WHO} 2016 guidelines were an important step forward for explaining the differences in glioma, there is still a lot ot discover about the mechanisms that drive the aggressiveness of glioma.
What developments will there be and how can these developments benefit radiomics methods?

An important development in the field of genomics is the commodity of more advanced genetic analysis machines.
With the introduction of next generation sequencing the results from the genetic analysis become more reliable, partially solving the problem of ground truth uncertainty, and also allows for more genes to be analyzed.
In this way, new genes can be discovered that correlate with the aggressiveness of the tumor, and in the future the categorization will most likely be solely dependent on molecular features.
This is beneficial for radiomics methods, since histological analysis is very observer-dependent, and molecular analysis less so.

In this thesis we assumed that the molecular and histological features of glioma were homogenous throughout the whole \gls{tumor}, this means that if a \gls{tumor} was considered for example \gls{IDH} wildtype, that the \gls{tumor} was \gls{IDH} wildtype everywhere.
However, it is known that the genetic and histological features can actually show intra-tumor heterogeneity \autocite{eder2014heterogeneity}.
Thus, radiomics methods need to be developed that take into account this intra-tumor heterogeneity.
This kind of radiomics, where instead of predicting single label for the whole \gls{tumor}, a label per voxel is predicted, is known as voxel-wise radimics.
Radiomics methods that do voxelwise classification using \gls{tumor} segmentations with different labels for different classes exist, which allow for a voxelwise prediction by training on homogeneous ground truth \autocite{yogananda20201p19q}.
Although these types of methods might be correct in their predictions, and could solve the problems that face the collection of the ground truth, it is at the moment impossible to accurately verify them.




\subsection{Who will get the data?}

Machine learning methods are only as good as the data that is used to develop and evaluate them.
Thus, databases have been ever increasing, containing larger amounts of and more varied data.
These clinical database are seen as the 'new gold',and a lot of parties are interested in these databaes.
However, all of this data comes from patients that were treated in a clinic.
Although, this data can help develop new methods that can improve healthcare for future patients, there are several questions surrounding the collection and sharing of this data.
Who gets the data?

Data is collected from routine clinical care or prospective studies.
Clinical data is privacy sensitive, and it requires special attention to ensure that none of this information can be traced back to the patient.
Furthermore, the patient might not want their data to be used apart from the clinical purpose.

The second question is one of finances.
Since both the Dutch healthcare system and the Dutch research infrastructure is largely financed by taxpayer money, it could be said that the since the data can improve the research and the research can cut down on healthcare costs, data needs to be collected and made available to all Dutch researchers.
However, what then if data is to be shared with institutes in other countries?
This data does contain some financial value, so given it away for free while it has been gathered using taxpayer money might be undesirable.
However, not sharing this data might lead to the stagnation of new developments, and might restrict people from having access to the best treatment.
Thus, this becomes an ethical dilemma.

One of the first iniatives that should be started is automated data collection within each institute.


\subsection{Should radiomics methods be unconstrained?}

As discussed in \cref{subsec:discussion_radiomics_performance}, radiomics methods are often compared with a golden standard ground truth.
However, these models might actually better predict the aggressiveness of \gls{glioma} than the ground truth molecular and histological features.
If enough data has been used to train a model, the model could pick up on image features that predict \gls{tumor} aggressiveness, uncorrelated with of the molecular features predicted.
For example, in \cref{chap:prognosais} some of patients were incorrectly predicted when considering the ground truth, but in the newest guidelines are actually correctly predicted, showing that the algorithm already picked up on this.
Thus, should radiomics methods be used without constraints?

In retrospective data one could link the predictions of the model with the survival of the patients, which might show patients in different groups than expected from the ground truth.
Ultimately, this could only be properly evaluated using a prospective study with stratified patients groups: one where the treatment is based on the radiomics model and one where the treatment is based on the golden standard ground truth.
However, this is a risky study, with little evidence being currently available that shows the potential benefits.

\subsection{What else can automated methods contribute?}

Radiomics methods often use the obtained scans as a start point of their method.
However, imaging already begins as soon as the patients steps into the scanner, and a lot of steps are taken before a clinical expert is presented with the scan.
In a lot of these steps automatic and machine learning methods can play a role, see \cref{fig:discussion_pipeline_automatic}.
A lot of these steps can be automated before a neuro-radiologist takes a first look at the scans, which can reduce the workload for the radiologist and can provide them with more consistent information by remove the observer-dependency, for example in the volume measurement of a \gls{glioma}.


\begin{figure}[htbp]
\includegraphics[width=\textwidth]{Figures/Pipeline.png}
\caption{Overview of some steps in which automated or machine learning methods can play a role from the scanner till the scan reaches the clinician}\label{fig:discussion_pipeline_automatic}
\end{figure}



\section{Conclusion}\label{sec:discussion_conclusion}


The above discussion raises a number of important questions that need to be addressed in future research.

Perhaps the most important question is what the future clinical application of radiomics should be.
Although radiomics methods show promising results, the resources used to develop them would be wasted if they do not eventually find a clinical application.
Thus, there are a few aveneus of approach that can be taken.

First of all, the view in which radiomics is normally presented: to obviate the need for an otherwise intrusive or difficult surgery.
In the case of glioma it seems unlikely that radiomics methods will replace biopsies or prevent surgeries.
The majority of the \glspl{tumor} are removed quickly after discovery, and the benefits of a watch-and-wait approach are often a topic of discussion.
Thus, the question becomes do the current radiomics methods provide enough additional benefit to warrant their existence, and if not, then what should they include to become relevant?
This question is difficult to answer, as a lot of different disciplines are involved in the treatment of glioma, each with their own view of what is needed to advance the patient treatment.
However, a consensus is required to steer the field of radiomics in the right direction.

As mentioned above, radiomics can also be viewed as a way of democratizing the latest  developments in glioma research.
This would require models to be made publicly available, and requires guidelines on when and how these radiomics models can be used if no golden standard method is available.
These guidelines should be made as approachable as possible, to make sure that the use of these models has a low threshold.


Therefore, there might also need to be a shift in the way radiomics is positioned.
Currently, it is often presented as a method that can replace a certain (currently used) golden standard method or even replace clinical experts.
Although this might become a reality in the long run, at the moment this seems rather unlikely.
Perhaps radiomics is better viewed as a way to democratize clinical expertise.
The methods can be developed using the latest insights and expertise available at privileged institutes, and this knowledge can then be distributed to institutes that normally would not have access to it.

All in all, radiomics and automated image analysis are definitely here to stay.
With the increasing healthcare costs, the increasing amounts of data, and the decreasing amount of time clinical experts can spend on a single patient, automatic methods offer a cheap solution that can easily deal with large amounts of data.
Although several methodological and social barriers still need to be overcome, the ultimate goal of improved patient healthcare should be kept as the focus both from the technological side, and from the clinical side.
Ultimately, we do need to move away from the human eye to more objective analysis methods, while at the same time making sure that these automated methods provide the information clinicians want and that the automated methods and clinical experts can see eye to eye about each other's value.

Check:
TODO: democratization
TODO: not all methods for all institutes (maybe in academic research goes too fast)
TODO: Other opportunities, made to fit with requirements everywhere
